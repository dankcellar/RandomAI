{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a72b6a78",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef13e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install trl\n",
    "# !pip install peft\n",
    "# !pip install matplotlib\n",
    "# !pip install accelerate\n",
    "# INSTALL TORCH MANUALLY\n",
    "# https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6ce1948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "\n",
    "def write_file(file_path, data):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        for datum in data:\n",
    "            f.write(datum)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aded7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# torch.set_default_device(\"cuda\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")\n",
    "# inputs = tokenizer(\n",
    "#     \"Write a detailed analogy between mathematics and a lighthouse.\", return_tensors=\"pt\", return_attention_mask=False\n",
    "# )\n",
    "\n",
    "# outputs = model.generate(**inputs, max_length=200)\n",
    "# text = tokenizer.batch_decode(outputs)[0]\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "155e2db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\D4nk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "def load_dataset(file_path, tokenizer, block_size=128):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=block_size,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_data_collator(tokenizer, mlm=False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=mlm,\n",
    "    )\n",
    "    return data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ef09c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST INDEX\n",
      "Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, Nine for Mortal Men doomed to die, One for the Dark Lord on his dark throne In the Land of Mordor where the Shadows lie.\n",
      "SECOND INDEX\n",
      "One Ring to rule them all, One Ring to find them, One Ring to bring them all and in the darkness bind them In the Land of Mordor where the Shadows lie.\n",
      "HOW MANY SENTENCES\n",
      "35506\n",
      "LONGEST SENTENCE\n",
      "631\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text_data_1 = read_txt(\"./lotr/the-fellowship-of-the-ring.txt\")\n",
    "text_data_2 = read_txt(\"./lotr/the-two-towers.txt\")\n",
    "text_data_3 = read_txt(\"./lotr/the-return-of-the-king.txt\")\n",
    "\n",
    "regex_pattern = r\"\\b[^.!?]+[.!?]+\"\n",
    "sentence_data_1 = re.findall(regex_pattern, text_data_1)\n",
    "sentence_data_2 = re.findall(regex_pattern, text_data_2)\n",
    "sentence_data_3 = re.findall(regex_pattern, text_data_3)\n",
    "sentence_data = sentence_data_1 + sentence_data_2 + sentence_data_3\n",
    "\n",
    "longest_sentence = 0\n",
    "min_words_in_sentence = 0  # 10\n",
    "max_string_sentence_len = 1024  # 512\n",
    "\n",
    "clean_data = list()\n",
    "for sentence in sentence_data:\n",
    "    word_list = sentence.split()\n",
    "    clean_sentence = \" \".join(word_list)\n",
    "\n",
    "    if len(word_list) > min_words_in_sentence:\n",
    "        if len(clean_sentence) < max_string_sentence_len:\n",
    "            clean_data.append(clean_sentence)\n",
    "\n",
    "    if len(clean_sentence) > longest_sentence:\n",
    "        longest_sentence = len(clean_sentence)\n",
    "\n",
    "write_file(\"./lotr/all.txt\", clean_data)\n",
    "\n",
    "print(\"FIRST INDEX\")\n",
    "print(clean_data[0])\n",
    "print(\"SECOND INDEX\")\n",
    "print(clean_data[1])\n",
    "print(\"HOW MANY SENTENCES\")\n",
    "print(len(clean_data))\n",
    "print(\"LONGEST SENTENCE\")\n",
    "print(longest_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "972ab991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TextDataset,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from transformers import AdamW\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset, concatenate_datasets, load_dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "import os\n",
    "import tempfile\n",
    "import gc\n",
    "import json\n",
    "\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"garbage_collection_threshold:0.6,max_split_size_mb:128\"\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"backend:cudaMallocAsync\"\n",
    "# os.environ[\"PYTORCH_NO_CUDA_MEMORY_CACHING\"] = \"1\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch.cuda.get_device_name())\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "# model_name = \"microsoft/phi-1_5\"\n",
    "model_name = \"bert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1d8e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\n",
    "#     \"text\",\n",
    "#     data_files={\n",
    "#         \"train\": [\n",
    "#             \"./lotr/all.txt\",\n",
    "#             # \"./lotr/the-fellowship-of-the-ring.txt\",\n",
    "#             # \"./lotr/the-return-of-the-king.txt\",\n",
    "#             # \"./lotr/the-two-towers.txt\",\n",
    "#         ]\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# encoded_dataset = dataset.map(\n",
    "#     lambda x: tokenizer.encode_plus(\n",
    "#         x[\"text\"],\n",
    "#         add_special_tokens=True,\n",
    "#         max_length=1024,\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#     ),\n",
    "#     batched=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d297b399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using eos_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b3b8ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to C:/Users/D4nk/.cache/huggingface/datasets/text/default-2d8d0ef27730de4f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 166.01it/s]\n",
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to C:/Users/D4nk/.cache/huggingface/datasets/text/default-2d8d0ef27730de4f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 99.98it/s]\n",
      "                                                                   \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 35506\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"text\", data_files={\"train\": [\"./lotr/all.txt\"]})\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(lambda examples: tokenize_function(examples))\n",
    "display(tokenized_train_dataset)\n",
    "\n",
    "# data = torch.utils.data.DataLoader(train_dataset, shuffle=True)\n",
    "# display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6309dc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertLMHeadModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=28996, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, trust_remote_code=True, torch_dtype=\"auto\"\n",
    ")  # , device_map=\"auto\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fc44499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\D4nk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/13317 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,token_type_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\RandomAI\\model-fine-tuning.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./output\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# num_train_epochs=3,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#X14sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39m# logging_steps=10,\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#X14sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     args\u001b[39m=\u001b[39margs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#X14sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39m# tokenizer=tokenizer,\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#X14sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#X14sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# model.train()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#X14sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# for epoch in range(10):\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#X14sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m#     index = 0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#X14sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m#         accelerator.backward(loss)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#X14sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m#         optimizer.step()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\D4nk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1630\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1632\u001b[0m )\n\u001b[1;32m-> 1633\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1634\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1635\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1636\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1637\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1638\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\D4nk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1902\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1900\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1901\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1902\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1904\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1905\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1906\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1907\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1908\u001b[0m ):\n\u001b[0;32m   1909\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1910\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\D4nk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2645\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2642\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   2644\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2645\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[0;32m   2647\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   2648\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\D4nk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2690\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2688\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2689\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m outputs:\n\u001b[1;32m-> 2690\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2691\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2692\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(outputs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m. For reference, the inputs it received are \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(inputs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2693\u001b[0m         )\n\u001b[0;32m   2694\u001b[0m     \u001b[39m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[0;32m   2695\u001b[0m     loss \u001b[39m=\u001b[39m outputs[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m outputs[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,token_type_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "### NOT USED\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "# train_loader = DataLoader(encoded_dataset[\"train\"], batch_size=8, shuffle=True)\n",
    "# eval_loader = DataLoader(encoded_dataset[\"test\"], batch_size=8)\n",
    "# optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "###\n",
    "\n",
    "### https://github.com/huggingface/accelerate/tree/main\n",
    "# model = torch.nn.Transformer()\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# dataset = load_dataset(\"my_dataset\")\n",
    "# data = torch.utils.data.DataLoader(tokenized_train_dataset[\"train\"], shuffle=False)\n",
    "\n",
    "# model, optimizer, data = accelerator.prepare(model, optimizer, data)\n",
    "###\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    # num_train_epochs=3,\n",
    "    # per_device_train_batch_size=8,\n",
    "    # per_device_eval_batch_size=8,\n",
    "    # warmup_steps=500,\n",
    "    # weight_decay=0.01,\n",
    "    # logging_dir=\"./logs\",\n",
    "    # logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train_dataset[\"train\"],\n",
    "    # eval_dataset=dataset[\"test\"],\n",
    "    # optimizers=optimizer\n",
    "    # data_collator=data_collator,\n",
    "    # tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# model.train()\n",
    "# for epoch in range(10):\n",
    "#     index = 0\n",
    "#     for datum in data:\n",
    "#         display(datum)\n",
    "#         # source, targets = datum\n",
    "#         # source = source.to(device)\n",
    "#         # targets = targets.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         # output = model(source)\n",
    "#         # loss = F.cross_entropy(output, targets)\n",
    "#         # datum[\"input_ids\"][index]\n",
    "#         output = model(datum[\"text\"])\n",
    "#         loss = F.cross_entropy(output, datum[\"input_ids\"][index])\n",
    "#         accelerator.backward(loss)\n",
    "#         optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbcb80b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b2f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, truncation=True, padding='max_length', return_special_tokens_mask=True)\n",
    "\n",
    "# text_df = pd.DataFrame({'Text':text})\n",
    "\n",
    "# # set up train and eval dataset\n",
    "# train_size=0.8\n",
    "# train_dataset = text_df.sample(frac=train_size,random_state=200)\n",
    "# test_dataset = text_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "# train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "# print(\"defined training and test set\")\n",
    "# def tokenize(text_df, tokenizer):\n",
    "#     tokenized_inputs = tokenizer(text_df[\"Text\"], is_split_into_words=False, padding='max_length',\n",
    "#                                  truncation=True,\n",
    "#                                  return_special_tokens_mask=True)# , return_tensors=\"pt\").to(device) #commented out bc gives errors\n",
    "#     return tokenized_inputs\n",
    "\n",
    "# train_data = Dataset.from_pandas(train_dataset).map(tokenize,\n",
    "#     fn_kwargs={'tokenizer':tokenizer},\n",
    "#     remove_columns=['Text'])\n",
    "# #train_data.set_format(\"torch\")\n",
    "# test_data = Dataset.from_pandas(test_dataset).map(tokenize,\n",
    "#     fn_kwargs={'tokenizer':tokenizer},\n",
    "#     remove_columns=['Text'])\n",
    "# #test_data.set_format(\"torch\")\n",
    "# print(\"tokenized data\")\n",
    "\n",
    "# test_labels = Dataset.from_pandas(pd.DataFrame({'labels':test_data['input_ids'].copy()}))\n",
    "# train_labels = Dataset.from_pandas(pd.DataFrame({'labels':train_data['input_ids'].copy()}))\n",
    "# test_data = concatenate_datasets([test_data, test_labels], axis=1)\n",
    "# train_data = concatenate_datasets([train_data, train_labels], axis=1)\n",
    "\n",
    "\n",
    "# #initiating model\n",
    "# model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "# model.to(device)\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer)\n",
    "# args = TrainingArguments(\n",
    "#     save_path),\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     learning_rate=1e-3,\n",
    "#     num_train_epochs=1,\n",
    "#     weight_decay=0.01,\n",
    "#     push_to_hub=False,\n",
    "#     per_device_train_batch_size = 8,#256,\n",
    "#     per_device_eval_batch_size = 8,#256,\n",
    "#     logging_steps=50,\n",
    "#     eval_steps = 50,\n",
    "#     save_total_limit = 3, #saves only last 3 checkpoints\n",
    "#     gradient_accumulation_steps=32,#64,\n",
    "#     gradient_checkpointing=True,\n",
    "#     fp16=True,\n",
    "#     optim=\"adafactor\"\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=args,\n",
    "#     train_dataset=train_data,\n",
    "#     eval_dataset=test_data,\n",
    "#     data_collator=data_collator,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "\n",
    "# train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf86b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_file_path = \"./lotr/the-fellowship-of-the-ring.txt\"\n",
    "# model_name = \"microsoft/phi-1_5\"\n",
    "# output_dir = \"./output\"\n",
    "# overwrite_output_dir = True\n",
    "# per_device_train_batch_size = 8\n",
    "# num_train_epochs = 50.0\n",
    "# save_steps = 50000\n",
    "\n",
    "# train(\n",
    "#     train_file_path=train_file_path,\n",
    "#     model_name=model_name,\n",
    "#     output_dir=output_dir,\n",
    "#     overwrite_output_dir=overwrite_output_dir,\n",
    "#     per_device_train_batch_size=per_device_train_batch_size,\n",
    "#     num_train_epochs=num_train_epochs,\n",
    "#     save_steps=save_steps,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef501948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_file_path = \"./lotr/the-fellowship-of-the-ring.txt\"\n",
    "# model_name = \"microsoft/phi-1_5\"\n",
    "# output_dir = \"./output\"\n",
    "# overwrite_output_dir = True\n",
    "# per_device_train_batch_size = 8\n",
    "# num_train_epochs = 50.0\n",
    "# save_steps = 50000\n",
    "\n",
    "# train(\n",
    "#     train_file_path=train_file_path,\n",
    "#     model_name=model_name,\n",
    "#     output_dir=output_dir,\n",
    "#     overwrite_output_dir=overwrite_output_dir,\n",
    "#     per_device_train_batch_size=per_device_train_batch_size,\n",
    "#     num_train_epochs=num_train_epochs,\n",
    "#     save_steps=save_steps,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9399ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\n",
    "#     \"text\",\n",
    "#     data_files={\n",
    "#         \"train\": [\n",
    "#             \"./lotr/the-fellowship-of-the-ring.txt\",\n",
    "#             \"./lotr/the-return-of-the-king.txt\",\n",
    "#             \"./lotr/the-two-towers.txt\",\n",
    "#         ]\n",
    "#     },\n",
    "# )\n",
    "\n",
    "\n",
    "# def load_dataset(file_path, tokenizer, block_size=128):\n",
    "#     dataset = TextDataset(\n",
    "#         tokenizer=tokenizer,\n",
    "#         file_path=file_path,\n",
    "#         block_size=block_size,\n",
    "#     )\n",
    "#     return dataset\n",
    "\n",
    "\n",
    "# print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "# tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "# small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "# small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c2db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer(\n",
    "#     \"write me a lord of the rings style short story\",\n",
    "#     return_tensors=\"pt\",\n",
    "#     return_attention_mask=False,\n",
    "# )\n",
    "# outputs = model.generate(**inputs, max_length=200)\n",
    "# text = tokenizer.batch_decode(outputs)[0]\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d03a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import (\n",
    "#     get_peft_config,\n",
    "#     get_peft_model,\n",
    "#     PromptTuningInit,\n",
    "#     PromptTuningConfig,\n",
    "#     TaskType,\n",
    "#     PeftType,\n",
    "#     PromptEncoderConfig,\n",
    "#     PrefixTuningConfig,\n",
    "#     LoraConfig,\n",
    "#     PeftConfig,\n",
    "# )\n",
    "\n",
    "# lora_peft_config = LoraConfig()\n",
    "\n",
    "# dynamic_padding = True\n",
    "\n",
    "\n",
    "# def tokenize_func(examples):\n",
    "#     return tokenizer(\n",
    "#         examples[\"input\"], truncation=True, max_length=1044\n",
    "#     )  # max_length=512,  padding=True\n",
    "\n",
    "\n",
    "# train_dataset_final = train_dataset.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc96dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "# model = prepare_model_for_int8_training(model)\n",
    "# model = get_peft_model(model, lora_peft_config)\n",
    "# training_args = model_training_args\n",
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     train_dataset=train_dataset_final,\n",
    "#     dataset_text_field=\"text\",\n",
    "#     max_seq_length=1044,\n",
    "#     tokenizer=tokenizer,\n",
    "#     args=model_training_args,\n",
    "#     packing=True,\n",
    "#     peft_config=lora_peft_config,\n",
    "# )\n",
    "# trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
