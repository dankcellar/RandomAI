{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef13e797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.27.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.12.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: datasets in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (1.25.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: trl in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from trl) (1.13.1)\n",
      "Requirement already satisfied: transformers>=4.18.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from trl) (4.27.2)\n",
      "Requirement already satisfied: numpy>=1.18.2 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from trl) (1.25.2)\n",
      "Requirement already satisfied: accelerate in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from trl) (0.22.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from trl) (2.11.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.4.0->trl) (4.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.18.0->trl) (3.12.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.18.0->trl) (0.16.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.18.0->trl) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.18.0->trl) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.18.0->trl) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.18.0->trl) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.18.0->trl) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers>=4.18.0->trl) (4.66.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate->trl) (5.9.5)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets->trl) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets->trl) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets->trl) (2.1.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets->trl) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets->trl) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets->trl) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets->trl) (3.8.5)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets->trl) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets->trl) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets->trl) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets->trl) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets->trl) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets->trl) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets->trl) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets->trl) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers>=4.18.0->trl) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers>=4.18.0->trl) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers>=4.18.0->trl) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers>=4.18.0->trl) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets->trl) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets->trl) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets->trl) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n",
      "Requirement already satisfied: torch in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: torchdata in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchdata) (2.0.4)\n",
      "Requirement already satisfied: requests in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchdata) (2.31.0)\n",
      "Requirement already satisfied: portalocker>=2.0.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchdata) (2.7.0)\n",
      "Requirement already satisfied: torch==1.13.1 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchdata) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==1.13.1->torchdata) (4.7.1)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\d4nk\\appdata\\roaming\\python\\python310\\site-packages (from portalocker>=2.0.0->torchdata) (306)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchdata) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchdata) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchdata) (2023.7.22)\n",
      "Requirement already satisfied: peft in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (23.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (1.13.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (4.27.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (4.66.1)\n",
      "Requirement already satisfied: accelerate in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (0.22.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from peft) (0.3.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13.0->peft) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers->peft) (3.12.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers->peft) (0.16.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers->peft) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers->peft) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers->peft) (0.13.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers->peft) (2023.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers->peft) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers->peft) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers->peft) (2023.7.22)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (10.0.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\d4nk\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install trl\n",
    "!pip install torch\n",
    "!pip install torchdata\n",
    "!pip install peft\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0add03a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# from datasets import load_dataset\n",
    "\n",
    "\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "# model_name: str = \"microsoft/phi-1_5\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name, trust_remote_code=True, torch_dtype=\"auto\"\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     model_name, trust_remote_code=True, torch_dtype=\"auto\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24c0807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to read different file types\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ef09c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text_data = read_txt(\"./lotr/the-fellowship-of-the-ring.txt\")\n",
    "text_data = re.sub(r\"\\n+\", \"\\n\", text_data).strip()  # Remove excess newline characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "972ab991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "def load_dataset(file_path, tokenizer, block_size=128):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=block_size,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_data_collator(tokenizer, mlm=False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=mlm,\n",
    "    )\n",
    "    return data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbe8804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_file_path,\n",
    "    model_name,\n",
    "    output_dir,\n",
    "    overwrite_output_dir,\n",
    "    per_device_train_batch_size,\n",
    "    num_train_epochs,\n",
    "    save_steps,\n",
    "):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    train_dataset = load_dataset(train_file_path, tokenizer)\n",
    "    data_collator = load_data_collator(tokenizer)\n",
    "\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=overwrite_output_dir,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fc44499",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = \"./lotr/the-fellowship-of-the-ring.txt\"\n",
    "model_name = \"microsoft/phi-1_5\"\n",
    "output_dir = \"./output\"\n",
    "overwrite_output_dir = False\n",
    "per_device_train_batch_size = 8\n",
    "num_train_epochs = 50.0\n",
    "save_steps = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4d615b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'CodeGenTokenizer'. \n",
      "The class this function is called from is 'GPT2Tokenizer'.\n",
      "c:\\Users\\D4nk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "You are using a model of type mixformer-sequential to instantiate a model of type gpt2. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at microsoft/phi-1_5 were not used when initializing GPT2LMHeadModel: ['layers.19.mixer.Wqkv.bias', 'layers.8.mlp.fc2.weight', 'layers.24.mlp.fc1.weight', 'layers.17.mlp.fc2.weight', 'layers.10.mlp.fc2.bias', 'layers.2.mlp.fc2.weight', 'layers.10.mixer.out_proj.bias', 'layers.16.ln.bias', 'layers.16.mixer.Wqkv.bias', 'layers.2.mixer.out_proj.weight', 'layers.8.mixer.out_proj.weight', 'layers.22.mlp.fc1.bias', 'layers.7.mixer.Wqkv.weight', 'layers.12.mixer.Wqkv.bias', 'layers.19.mixer.Wqkv.weight', 'layers.6.mixer.Wqkv.bias', 'layers.18.mlp.fc2.bias', 'layers.13.mixer.Wqkv.weight', 'layers.15.mixer.out_proj.weight', 'layers.21.mixer.Wqkv.weight', 'layers.9.mlp.fc1.bias', 'layers.15.ln.bias', 'layers.22.mixer.Wqkv.weight', 'layers.5.mlp.fc1.bias', 'layers.10.mixer.Wqkv.bias', 'layers.5.mlp.fc2.bias', 'layers.24.mlp.fc2.weight', 'layers.20.mixer.Wqkv.bias', 'layers.1.mixer.Wqkv.bias', 'layers.16.mixer.out_proj.weight', 'layers.23.mixer.Wqkv.bias', 'layers.24.mixer.out_proj.bias', 'layers.18.mixer.Wqkv.weight', 'layers.18.mixer.Wqkv.bias', 'layers.3.ln.weight', 'layers.10.mlp.fc1.bias', 'layers.8.mixer.rotary_emb.inv_freq', 'layers.11.ln.bias', 'layers.17.mlp.fc1.weight', 'layers.18.ln.weight', 'layers.11.mlp.fc1.weight', 'layers.19.mixer.rotary_emb.inv_freq', 'layers.10.mixer.out_proj.weight', 'layers.20.ln.bias', 'layers.18.mixer.out_proj.weight', 'layers.7.mlp.fc1.weight', 'layers.22.mixer.rotary_emb.inv_freq', 'layers.4.mixer.rotary_emb.inv_freq', 'layers.23.mlp.fc1.weight', 'layers.8.mlp.fc2.bias', 'layers.23.mlp.fc1.bias', 'layers.24.mixer.Wqkv.weight', 'layers.12.mlp.fc2.weight', 'layers.9.mixer.out_proj.bias', 'layers.2.mlp.fc1.bias', 'layers.20.ln.weight', 'layers.18.mixer.out_proj.bias', 'layers.23.mixer.rotary_emb.inv_freq', 'layers.15.mixer.Wqkv.bias', 'layers.14.mlp.fc1.bias', 'layers.17.ln.weight', 'layers.20.mlp.fc2.bias', 'layers.2.mixer.out_proj.bias', 'layers.13.mixer.out_proj.weight', 'layers.19.mixer.out_proj.weight', 'layers.24.ln.weight', 'layers.15.mixer.rotary_emb.inv_freq', 'layers.11.mixer.out_proj.weight', 'layers.25.linear.bias', 'layers.6.mixer.out_proj.weight', 'layers.23.mixer.out_proj.weight', 'layers.8.ln.weight', 'layers.6.mixer.rotary_emb.inv_freq', 'layers.9.mixer.Wqkv.bias', 'layers.23.ln.bias', 'layers.24.mixer.rotary_emb.inv_freq', 'layers.7.mlp.fc1.bias', 'layers.13.mlp.fc1.weight', 'layers.17.mixer.Wqkv.weight', 'layers.14.mixer.out_proj.bias', 'layers.18.mixer.rotary_emb.inv_freq', 'layers.3.mixer.rotary_emb.inv_freq', 'layers.6.mixer.Wqkv.weight', 'layers.6.mlp.fc1.weight', 'layers.17.mixer.Wqkv.bias', 'layers.21.mlp.fc2.weight', 'layers.12.mixer.rotary_emb.inv_freq', 'layers.21.ln.weight', 'layers.1.ln.bias', 'layers.11.mixer.Wqkv.weight', 'layers.14.mixer.out_proj.weight', 'layers.3.ln.bias', 'layers.21.mlp.fc1.bias', 'layers.16.mixer.out_proj.bias', 'layers.11.mlp.fc1.bias', 'layers.13.mlp.fc2.bias', 'layers.11.mixer.Wqkv.bias', 'layers.21.mixer.Wqkv.bias', 'layers.3.mlp.fc2.bias', 'layers.17.ln.bias', 'layers.15.ln.weight', 'layers.16.mlp.fc2.bias', 'layers.19.mlp.fc2.weight', 'layers.22.mlp.fc2.bias', 'layers.10.mixer.rotary_emb.inv_freq', 'layers.8.mixer.Wqkv.weight', 'layers.4.ln.weight', 'layers.9.ln.bias', 'layers.7.mlp.fc2.weight', 'layers.23.mlp.fc2.weight', 'layers.17.mixer.out_proj.weight', 'layers.0.wte.weight', 'layers.13.mixer.rotary_emb.inv_freq', 'layers.22.mlp.fc2.weight', 'layers.24.mixer.Wqkv.bias', 'layers.11.mixer.out_proj.bias', 'layers.23.mixer.out_proj.bias', 'layers.6.mlp.fc2.bias', 'layers.12.ln.weight', 'layers.12.mlp.fc1.bias', 'layers.7.mlp.fc2.bias', 'layers.12.mixer.Wqkv.weight', 'layers.22.mixer.Wqkv.bias', 'layers.7.mixer.out_proj.bias', 'layers.10.ln.weight', 'layers.1.mlp.fc2.bias', 'layers.2.mixer.Wqkv.bias', 'layers.3.mixer.Wqkv.bias', 'layers.20.mlp.fc1.bias', 'layers.21.mixer.rotary_emb.inv_freq', 'layers.12.mlp.fc2.bias', 'layers.3.mixer.Wqkv.weight', 'layers.16.mlp.fc1.weight', 'layers.3.mixer.out_proj.weight', 'layers.16.ln.weight', 'layers.10.ln.bias', 'layers.10.mixer.Wqkv.weight', 'layers.9.mixer.out_proj.weight', 'layers.8.ln.bias', 'layers.15.mixer.Wqkv.weight', 'layers.12.mlp.fc1.weight', 'layers.21.mixer.out_proj.weight', 'layers.3.mlp.fc2.weight', 'layers.13.mlp.fc1.bias', 'layers.6.ln.weight', 'layers.2.mixer.rotary_emb.inv_freq', 'layers.5.mixer.out_proj.weight', 'layers.17.mlp.fc2.bias', 'layers.2.ln.weight', 'layers.18.ln.bias', 'layers.4.mixer.out_proj.bias', 'layers.13.mlp.fc2.weight', 'layers.15.mlp.fc1.bias', 'layers.13.ln.weight', 'layers.22.mlp.fc1.weight', 'layers.23.ln.weight', 'layers.12.ln.bias', 'layers.1.mixer.out_proj.weight', 'layers.20.mlp.fc1.weight', 'layers.24.mlp.fc1.bias', 'layers.6.mlp.fc1.bias', 'layers.1.mlp.fc1.bias', 'layers.4.mixer.Wqkv.weight', 'layers.14.ln.weight', 'layers.14.mixer.Wqkv.weight', 'layers.16.mixer.Wqkv.weight', 'layers.17.mixer.rotary_emb.inv_freq', 'layers.24.mlp.fc2.bias', 'layers.2.ln.bias', 'layers.8.mlp.fc1.weight', 'layers.19.ln.weight', 'layers.2.mlp.fc2.bias', 'layers.15.mlp.fc2.bias', 'layers.6.mixer.out_proj.bias', 'layers.20.mixer.out_proj.bias', 'layers.6.ln.bias', 'layers.3.mixer.out_proj.bias', 'layers.4.mlp.fc1.bias', 'layers.5.ln.bias', 'layers.2.mixer.Wqkv.weight', 'layers.16.mixer.rotary_emb.inv_freq', 'layers.25.ln.bias', 'layers.1.mlp.fc1.weight', 'layers.13.ln.bias', 'layers.5.mixer.out_proj.bias', 'layers.9.mixer.rotary_emb.inv_freq', 'layers.19.mlp.fc1.bias', 'layers.16.mlp.fc1.bias', 'layers.19.ln.bias', 'layers.7.ln.weight', 'layers.13.mixer.out_proj.bias', 'layers.20.mixer.rotary_emb.inv_freq', 'layers.4.mixer.Wqkv.bias', 'layers.5.ln.weight', 'layers.21.mlp.fc2.bias', 'layers.6.mlp.fc2.weight', 'layers.2.mlp.fc1.weight', 'layers.9.mixer.Wqkv.weight', 'layers.9.mlp.fc1.weight', 'layers.7.mixer.out_proj.weight', 'layers.13.mixer.Wqkv.bias', 'layers.17.mlp.fc1.bias', 'layers.21.mlp.fc1.weight', 'layers.23.mlp.fc2.bias', 'layers.9.ln.weight', 'layers.14.mlp.fc2.bias', 'layers.15.mlp.fc1.weight', 'layers.22.ln.weight', 'layers.8.mixer.Wqkv.bias', 'layers.4.mlp.fc2.weight', 'layers.4.mlp.fc1.weight', 'layers.22.mixer.out_proj.weight', 'layers.5.mlp.fc1.weight', 'layers.21.ln.bias', 'layers.3.mlp.fc1.bias', 'layers.4.mixer.out_proj.weight', 'layers.20.mixer.out_proj.weight', 'layers.12.mixer.out_proj.weight', 'layers.21.mixer.out_proj.bias', 'layers.14.mixer.Wqkv.bias', 'layers.14.mixer.rotary_emb.inv_freq', 'layers.18.mlp.fc1.bias', 'layers.9.mlp.fc2.weight', 'layers.1.mixer.rotary_emb.inv_freq', 'layers.7.ln.bias', 'layers.11.mixer.rotary_emb.inv_freq', 'layers.7.mixer.rotary_emb.inv_freq', 'layers.5.mixer.Wqkv.bias', 'layers.8.mixer.out_proj.bias', 'layers.8.mlp.fc1.bias', 'layers.11.ln.weight', 'layers.20.mlp.fc2.weight', 'layers.15.mixer.out_proj.bias', 'layers.14.mlp.fc1.weight', 'layers.24.mixer.out_proj.weight', 'layers.14.mlp.fc2.weight', 'layers.20.mixer.Wqkv.weight', 'layers.19.mixer.out_proj.bias', 'layers.4.ln.bias', 'layers.18.mlp.fc2.weight', 'layers.22.mixer.out_proj.bias', 'layers.12.mixer.out_proj.bias', 'layers.4.mlp.fc2.bias', 'layers.15.mlp.fc2.weight', 'layers.1.ln.weight', 'layers.1.mixer.out_proj.bias', 'layers.14.ln.bias', 'layers.22.ln.bias', 'layers.11.mlp.fc2.bias', 'layers.10.mlp.fc2.weight', 'layers.5.mlp.fc2.weight', 'layers.23.mixer.Wqkv.weight', 'layers.18.mlp.fc1.weight', 'layers.10.mlp.fc1.weight', 'layers.3.mlp.fc1.weight', 'layers.17.mixer.out_proj.bias', 'layers.1.mlp.fc2.weight', 'layers.9.mlp.fc2.bias', 'layers.7.mixer.Wqkv.bias', 'layers.11.mlp.fc2.weight', 'layers.1.mixer.Wqkv.weight', 'layers.19.mlp.fc1.weight', 'layers.5.mixer.Wqkv.weight', 'layers.19.mlp.fc2.bias', 'layers.16.mlp.fc2.weight', 'layers.25.linear.weight', 'layers.24.ln.bias', 'layers.25.ln.weight', 'layers.5.mixer.rotary_emb.inv_freq']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at microsoft/phi-1_5 and are newly initialized: ['h.1.attn.c_proj.weight', 'h.17.mlp.c_fc.weight', 'h.7.ln_2.weight', 'h.14.attn.c_attn.weight', 'h.5.attn.c_proj.bias', 'h.9.mlp.c_fc.bias', 'h.7.mlp.c_proj.weight', 'h.0.ln_1.bias', 'h.4.mlp.c_proj.bias', 'h.10.mlp.c_proj.bias', 'h.20.attn.c_attn.weight', 'h.22.ln_2.bias', 'h.0.ln_1.weight', 'h.18.attn.c_proj.bias', 'h.8.mlp.c_proj.bias', 'h.12.attn.c_proj.bias', 'h.2.mlp.c_fc.bias', 'h.1.ln_2.bias', 'h.8.mlp.c_proj.weight', 'h.19.ln_2.bias', 'h.21.ln_1.bias', 'h.5.ln_1.weight', 'h.21.attn.c_proj.bias', 'h.22.ln_1.bias', 'h.2.ln_2.weight', 'h.20.mlp.c_fc.weight', 'h.15.attn.c_attn.weight', 'h.17.ln_1.bias', 'h.23.attn.c_proj.bias', 'h.23.mlp.c_fc.bias', 'h.20.mlp.c_fc.bias', 'h.6.attn.c_proj.weight', 'h.2.ln_2.bias', 'h.5.attn.c_attn.weight', 'h.5.attn.c_proj.weight', 'h.17.attn.c_attn.weight', 'h.15.mlp.c_proj.bias', 'h.17.attn.c_proj.weight', 'h.16.ln_1.weight', 'h.8.mlp.c_fc.bias', 'h.8.mlp.c_fc.weight', 'h.11.mlp.c_proj.weight', 'h.15.ln_2.bias', 'h.15.ln_2.weight', 'h.10.ln_1.bias', 'h.4.attn.c_proj.weight', 'h.14.ln_2.weight', 'h.10.mlp.c_fc.bias', 'h.13.mlp.c_proj.weight', 'h.1.ln_2.weight', 'h.3.attn.c_attn.weight', 'h.7.ln_1.bias', 'h.8.ln_2.bias', 'h.2.mlp.c_fc.weight', 'h.23.attn.c_attn.weight', 'h.1.ln_1.weight', 'h.0.ln_2.bias', 'wpe.weight', 'h.21.mlp.c_proj.bias', 'h.6.attn.c_proj.bias', 'h.14.attn.c_proj.weight', 'h.10.attn.c_proj.bias', 'h.15.attn.c_proj.bias', 'h.13.mlp.c_fc.bias', 'h.3.mlp.c_proj.weight', 'h.0.ln_2.weight', 'h.22.attn.c_attn.weight', 'h.23.ln_1.bias', 'h.9.mlp.c_proj.weight', 'h.18.ln_2.bias', 'h.9.mlp.c_fc.weight', 'h.21.mlp.c_fc.weight', 'h.22.ln_1.weight', 'h.4.ln_2.weight', 'h.22.mlp.c_proj.weight', 'h.18.ln_2.weight', 'h.2.mlp.c_proj.bias', 'h.5.mlp.c_proj.bias', 'h.2.ln_1.weight', 'h.13.mlp.c_proj.bias', 'h.12.ln_1.bias', 'h.9.attn.c_proj.bias', 'h.5.mlp.c_fc.weight', 'h.19.ln_1.weight', 'h.11.mlp.c_fc.bias', 'h.13.ln_2.weight', 'ln_f.bias', 'h.4.mlp.c_proj.weight', 'h.12.mlp.c_fc.bias', 'h.14.attn.c_proj.bias', 'h.10.ln_2.weight', 'h.21.ln_2.weight', 'h.0.mlp.c_proj.weight', 'h.0.mlp.c_fc.weight', 'h.12.attn.c_attn.weight', 'h.6.attn.c_attn.weight', 'h.10.attn.c_attn.weight', 'h.16.mlp.c_fc.weight', 'h.11.ln_2.bias', 'h.1.mlp.c_fc.bias', 'h.10.attn.c_proj.weight', 'h.22.attn.c_proj.bias', 'h.14.ln_1.bias', 'h.6.ln_1.weight', 'h.22.ln_2.weight', 'h.9.ln_2.bias', 'h.6.ln_2.bias', 'h.20.mlp.c_proj.weight', 'h.20.ln_2.bias', 'h.18.attn.c_attn.weight', 'h.5.ln_1.bias', 'h.3.ln_1.weight', 'h.2.attn.c_proj.weight', 'h.16.ln_1.bias', 'h.16.ln_2.weight', 'h.6.mlp.c_proj.weight', 'h.4.mlp.c_fc.weight', 'h.9.ln_1.bias', 'h.19.attn.c_attn.weight', 'h.7.mlp.c_proj.bias', 'h.16.mlp.c_fc.bias', 'h.23.attn.c_proj.weight', 'h.10.ln_2.bias', 'h.15.attn.c_proj.weight', 'h.19.ln_1.bias', 'h.20.mlp.c_proj.bias', 'h.23.mlp.c_proj.bias', 'h.18.ln_1.weight', 'h.6.mlp.c_proj.bias', 'h.1.ln_1.bias', 'h.7.mlp.c_fc.bias', 'h.11.ln_1.bias', 'h.19.ln_2.weight', 'h.11.ln_1.weight', 'h.15.mlp.c_fc.bias', 'h.18.mlp.c_proj.bias', 'h.23.ln_2.bias', 'h.14.ln_2.bias', 'h.13.mlp.c_fc.weight', 'h.18.mlp.c_fc.bias', 'h.12.ln_2.weight', 'h.19.mlp.c_proj.bias', 'h.20.ln_1.bias', 'h.13.ln_1.bias', 'h.15.mlp.c_fc.weight', 'h.5.ln_2.bias', 'h.6.ln_2.weight', 'h.18.mlp.c_proj.weight', 'h.18.attn.c_proj.weight', 'h.1.mlp.c_proj.bias', 'h.1.attn.c_proj.bias', 'h.10.mlp.c_fc.weight', 'h.3.attn.c_proj.bias', 'h.1.mlp.c_proj.weight', 'h.21.ln_1.weight', 'h.11.attn.c_proj.weight', 'h.22.mlp.c_fc.bias', 'h.3.ln_2.weight', 'h.2.attn.c_attn.weight', 'h.19.mlp.c_fc.bias', 'h.2.attn.c_proj.bias', 'h.22.mlp.c_proj.bias', 'h.17.mlp.c_fc.bias', 'h.8.attn.c_proj.bias', 'h.18.mlp.c_fc.weight', 'h.11.mlp.c_proj.bias', 'h.16.attn.c_proj.weight', 'h.20.attn.c_proj.bias', 'h.3.attn.c_proj.weight', 'h.3.mlp.c_fc.weight', 'h.17.mlp.c_proj.weight', 'h.7.ln_2.bias', 'h.15.ln_1.weight', 'h.9.ln_2.weight', 'h.11.mlp.c_fc.weight', 'h.7.mlp.c_fc.weight', 'h.22.attn.c_proj.weight', 'h.5.mlp.c_fc.bias', 'h.16.mlp.c_proj.weight', 'h.21.ln_2.bias', 'h.19.mlp.c_proj.weight', 'h.4.ln_2.bias', 'h.12.ln_2.bias', 'h.9.attn.c_attn.weight', 'h.4.ln_1.weight', 'h.12.mlp.c_fc.weight', 'h.0.attn.c_proj.bias', 'h.0.mlp.c_proj.bias', 'h.14.ln_1.weight', 'h.8.ln_1.weight', 'h.17.attn.c_proj.bias', 'h.9.mlp.c_proj.bias', 'h.23.ln_2.weight', 'h.13.attn.c_proj.bias', 'h.12.ln_1.weight', 'h.10.ln_1.weight', 'h.23.mlp.c_fc.weight', 'h.14.mlp.c_proj.weight', 'h.2.ln_1.bias', 'h.3.mlp.c_proj.bias', 'h.20.ln_2.weight', 'h.23.ln_1.weight', 'h.22.mlp.c_fc.weight', 'h.15.ln_1.bias', 'h.4.mlp.c_fc.bias', 'h.13.ln_1.weight', 'h.3.ln_2.bias', 'h.17.mlp.c_proj.bias', 'h.5.ln_2.weight', 'h.10.mlp.c_proj.weight', 'h.13.attn.c_attn.weight', 'h.19.mlp.c_fc.weight', 'h.7.attn.c_proj.weight', 'h.0.mlp.c_fc.bias', 'h.21.mlp.c_fc.bias', 'h.14.mlp.c_fc.bias', 'h.17.ln_2.bias', 'h.7.ln_1.weight', 'h.16.attn.c_proj.bias', 'h.17.ln_2.weight', 'h.9.attn.c_proj.weight', 'h.19.attn.c_proj.weight', 'h.8.attn.c_attn.weight', 'h.16.attn.c_attn.weight', 'h.23.mlp.c_proj.weight', 'h.11.attn.c_proj.bias', 'h.18.ln_1.bias', 'h.20.attn.c_proj.weight', 'h.0.attn.c_attn.weight', 'h.6.mlp.c_fc.bias', 'h.7.attn.c_attn.weight', 'h.8.ln_1.bias', 'h.1.attn.c_attn.weight', 'h.8.attn.c_proj.weight', 'wte.weight', 'h.14.mlp.c_proj.bias', 'h.2.mlp.c_proj.weight', 'h.8.ln_2.weight', 'h.9.ln_1.weight', 'h.4.attn.c_attn.weight', 'h.16.ln_2.bias', 'h.21.mlp.c_proj.weight', 'h.12.attn.c_proj.weight', 'h.21.attn.c_proj.weight', 'h.3.ln_1.bias', 'h.13.ln_2.bias', 'h.6.ln_1.bias', 'h.4.attn.c_proj.bias', 'h.3.mlp.c_fc.bias', 'h.4.ln_1.bias', 'h.0.attn.c_proj.weight', 'h.12.mlp.c_proj.bias', 'h.6.mlp.c_fc.weight', 'h.20.ln_1.weight', 'h.5.mlp.c_proj.weight', 'h.13.attn.c_proj.weight', 'h.1.mlp.c_fc.weight', 'h.12.mlp.c_proj.weight', 'h.15.mlp.c_proj.weight', 'ln_f.weight', 'h.16.mlp.c_proj.bias', 'h.11.attn.c_attn.weight', 'h.14.mlp.c_fc.weight', 'h.11.ln_2.weight', 'h.7.attn.c_proj.bias', 'h.17.ln_1.weight', 'h.21.attn.c_attn.weight', 'h.19.attn.c_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\D4nk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 14/12000 [15:57<207:38:17, 62.36s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\RandomAI\\add to model.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train_file_path\u001b[39m=\u001b[39;49mtrain_file_path,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model_name\u001b[39m=\u001b[39;49mmodel_name,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     output_dir\u001b[39m=\u001b[39;49moutput_dir,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     overwrite_output_dir\u001b[39m=\u001b[39;49moverwrite_output_dir,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39;49mper_device_train_batch_size,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39;49mnum_train_epochs,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     save_steps\u001b[39m=\u001b[39;49msave_steps,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\n",
      "\u001b[1;32md:\\Projects\\RandomAI\\add to model.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     output_dir\u001b[39m=\u001b[39moutput_dir,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     overwrite_output_dir\u001b[39m=\u001b[39moverwrite_output_dir,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39mper_device_train_batch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#X20sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39mnum_train_epochs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#X20sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#X20sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#X20sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#X20sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     data_collator\u001b[39m=\u001b[39mdata_collator,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#X20sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mtrain_dataset,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#X20sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#X20sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#X20sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m trainer\u001b[39m.\u001b[39msave_model()\n",
      "File \u001b[1;32mc:\\Users\\D4nk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1630\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1632\u001b[0m )\n\u001b[1;32m-> 1633\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1634\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1635\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1636\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1637\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1638\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\D4nk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1902\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1900\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1901\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1902\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1904\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1905\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1906\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1907\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1908\u001b[0m ):\n\u001b[0;32m   1909\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1910\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\D4nk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2645\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2642\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   2644\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2645\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[0;32m   2647\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   2648\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\D4nk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2677\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2675\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2676\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2677\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m   2678\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2679\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2680\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\D4nk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\D4nk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1102\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1099\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m     \u001b[39m# Shift so that tokens < n predict n\u001b[39;00m\n\u001b[1;32m-> 1102\u001b[0m     shift_logits \u001b[39m=\u001b[39m lm_logits[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m, :\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, :]\u001b[39m.\u001b[39;49mcontiguous()\n\u001b[0;32m   1103\u001b[0m     shift_labels \u001b[39m=\u001b[39m labels[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m   1104\u001b[0m     \u001b[39m# Flatten the tokens\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "train(\n",
    "    train_file_path=train_file_path,\n",
    "    model_name=model_name,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a9399ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_dataset() got an unexpected keyword argument 'data_files'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\RandomAI\\add to model.ipynb Cell 7\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m text_data \u001b[39m=\u001b[39m read_txt(\u001b[39m\"\u001b[39m\u001b[39m./lotr/the-fellowship-of-the-ring.txt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m text_data \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mn+\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, text_data)\u001b[39m.\u001b[39mstrip()  \u001b[39m# Remove excess newline characters\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     data_files\u001b[39m=\u001b[39;49m{\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m: [\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39m./lotr/the-fellowship-of-the-ring.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39m./lotr/the-return-of-the-king.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39m./lotr/the-two-towers.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         ]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     },\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_dataset\u001b[39m(file_path, tokenizer, block_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     dataset \u001b[39m=\u001b[39m TextDataset(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         file_path\u001b[39m=\u001b[39mfile_path,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         block_size\u001b[39m=\u001b[39mblock_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/add%20to%20model.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: load_dataset() got an unexpected keyword argument 'data_files'"
     ]
    }
   ],
   "source": [
    "# dataset = load_dataset(\n",
    "#     \"text\",\n",
    "#     data_files={\n",
    "#         \"train\": [\n",
    "#             \"./lotr/the-fellowship-of-the-ring.txt\",\n",
    "#             \"./lotr/the-return-of-the-king.txt\",\n",
    "#             \"./lotr/the-two-towers.txt\",\n",
    "#         ]\n",
    "#     },\n",
    "# )\n",
    "\n",
    "\n",
    "# def load_dataset(file_path, tokenizer, block_size=128):\n",
    "#     dataset = TextDataset(\n",
    "#         tokenizer=tokenizer,\n",
    "#         file_path=file_path,\n",
    "#         block_size=block_size,\n",
    "#     )\n",
    "#     return dataset\n",
    "\n",
    "\n",
    "# print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "# tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "# small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "# small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c2db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write me a lord of the rings style short story.\n",
      "\n",
      "Lily: Sure, I can do that. But I have to warn you, I'm not very good at writing fantasy stories. I usually write realistic fiction.\n",
      "\n",
      "Emma: That's okay, I'm sure you can do it. I'm sure you're a great writer.\n",
      "\n",
      "Lily: Thanks, Emma. You're very kind. But I'm also a bit nervous. I don't know if I can come up with a good idea.\n",
      "\n",
      "Emma: Don't worry, you'll think of something. Just use your imagination. Think of a world that's different from ours. Think of a problem that people face. Think of a hero who saves the day.\n",
      "\n",
      "Lily: OK, I'll try. But what if I can't think of anything? What if I can't write anything?\n",
      "\n",
      "Emma: Don't think like that. You're a creative person\n"
     ]
    }
   ],
   "source": [
    "# inputs = tokenizer(\n",
    "#     \"write me a lord of the rings style short story\",\n",
    "#     return_tensors=\"pt\",\n",
    "#     return_attention_mask=False,\n",
    "# )\n",
    "# outputs = model.generate(**inputs, max_length=200)\n",
    "# text = tokenizer.batch_decode(outputs)[0]\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d03a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    PromptTuningInit,\n",
    "    PromptTuningConfig,\n",
    "    TaskType,\n",
    "    PeftType,\n",
    "    PromptEncoderConfig,\n",
    "    PrefixTuningConfig,\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    ")\n",
    "\n",
    "lora_peft_config = LoraConfig()\n",
    "\n",
    "dynamic_padding = True\n",
    "\n",
    "\n",
    "def tokenize_func(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"input\"], truncation=True, max_length=1044\n",
    "    )  # max_length=512,  padding=True\n",
    "\n",
    "\n",
    "train_dataset_final = train_dataset.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc96dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = prepare_model_for_int8_training(model)\n",
    "model = get_peft_model(model, lora_peft_config)\n",
    "training_args = model_training_args\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset_final,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1044,\n",
    "    tokenizer=tokenizer,\n",
    "    args=model_training_args,\n",
    "    packing=True,\n",
    "    peft_config=lora_peft_config,\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
