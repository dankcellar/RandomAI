{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef13e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install trl\n",
    "# !pip install torch\n",
    "# !pip install torchdata\n",
    "# !pip install peft\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0add03a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# from datasets import load_dataset\n",
    "\n",
    "\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "# model_name: str = \"microsoft/phi-1_5\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name, trust_remote_code=True, torch_dtype=\"auto\"\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     model_name, trust_remote_code=True, torch_dtype=\"auto\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24c0807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "\n",
    "def write_file(file_path, data):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        for datum in data:\n",
    "            f.write(datum)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "155e2db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "def load_dataset(file_path, tokenizer, block_size=128):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=block_size,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_data_collator(tokenizer, mlm=False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=mlm,\n",
    "    )\n",
    "    return data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ef09c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST INDEX\n",
      "Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, Nine for Mortal Men doomed to die, One for the Dark Lord on his dark throne In the Land of Mordor where the Shadows lie.\n",
      "SECOND INDEX\n",
      "One Ring to rule them all, One Ring to find them, One Ring to bring them all and in the darkness bind them In the Land of Mordor where the Shadows lie.\n",
      "HOW MANY SENTENCES\n",
      "35506\n",
      "LONGEST SENTENCE\n",
      "631\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text_data_1 = read_txt(\"./lotr/the-fellowship-of-the-ring.txt\")\n",
    "text_data_2 = read_txt(\"./lotr/the-two-towers.txt\")\n",
    "text_data_3 = read_txt(\"./lotr/the-return-of-the-king.txt\")\n",
    "\n",
    "regex_pattern = r\"\\b[^.!?]+[.!?]+\"\n",
    "sentence_data_1 = re.findall(regex_pattern, text_data_1)\n",
    "sentence_data_2 = re.findall(regex_pattern, text_data_2)\n",
    "sentence_data_3 = re.findall(regex_pattern, text_data_3)\n",
    "sentence_data = sentence_data_1 + sentence_data_2 + sentence_data_3\n",
    "\n",
    "longest_sentence = 0\n",
    "min_words_in_sentence = 0  # 10\n",
    "max_string_sentence_len = 1024  # 512\n",
    "\n",
    "clean_data = list()\n",
    "for sentence in sentence_data:\n",
    "    word_list = sentence.split()\n",
    "    clean_sentence = \" \".join(word_list)\n",
    "\n",
    "    if len(word_list) > min_words_in_sentence:\n",
    "        if len(clean_sentence) < max_string_sentence_len:\n",
    "            clean_data.append(clean_sentence)\n",
    "\n",
    "    if len(clean_sentence) > longest_sentence:\n",
    "        longest_sentence = len(clean_sentence)\n",
    "\n",
    "write_file(\"./lotr/all.txt\", clean_data)\n",
    "\n",
    "print(\"FIRST INDEX\")\n",
    "print(clean_data[0])\n",
    "print(\"SECOND INDEX\")\n",
    "print(clean_data[1])\n",
    "print(\"HOW MANY SENTENCES\")\n",
    "print(len(clean_data))\n",
    "print(\"LONGEST SENTENCE\")\n",
    "print(longest_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "972ab991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TextDataset,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1d8e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\n",
    "#     \"text\",\n",
    "#     data_files={\n",
    "#         \"train\": [\n",
    "#             \"./lotr/all.txt\",\n",
    "#             # \"./lotr/the-fellowship-of-the-ring.txt\",\n",
    "#             # \"./lotr/the-return-of-the-king.txt\",\n",
    "#             # \"./lotr/the-two-towers.txt\",\n",
    "#         ]\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# encoded_dataset = dataset.map(\n",
    "#     lambda x: tokenizer.encode_plus(\n",
    "#         x[\"text\"],\n",
    "#         add_special_tokens=True,\n",
    "#         max_length=1024,\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#     ),\n",
    "#     batched=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fc44499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.data.datasets.language_modeling.TextDataset object at 0x0000011C36D1C850>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\RandomAI\\model-fine-tuning.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#W6sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./output\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#W6sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39m# num_train_epochs=3,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#W6sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39m# logging_steps=10,\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#W6sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#W6sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#W6sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#W6sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#W6sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39m# eval_dataset=dataset[\"test\"],\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#W6sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects/RandomAI/model-fine-tuning.ipynb#W6sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\D4nk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1630\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1632\u001b[0m )\n\u001b[1;32m-> 1633\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1634\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1635\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1636\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1637\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1638\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\D4nk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1902\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1900\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1901\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1902\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1904\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1905\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1906\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1907\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1908\u001b[0m ):\n\u001b[0;32m   1909\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1910\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\D4nk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2645\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2642\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   2644\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2645\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[0;32m   2647\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   2648\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\D4nk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2677\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2675\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2676\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2677\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m   2678\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2679\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2680\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\D4nk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\microsoft\\phi-1_5\\4a426d8015bef5a0cb3acff8d4474ee9ab4071d5\\modeling_mixformer_sequential.py:768\u001b[0m, in \u001b[0;36mMixFormerSequentialForCausalLM.forward\u001b[1;34m(self, input_ids, labels, past_key_values, **kwargs)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    763\u001b[0m     \u001b[39mself\u001b[39m, input_ids: torch\u001b[39m.\u001b[39mLongTensor, labels: Optional[torch\u001b[39m.\u001b[39mLongTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \n\u001b[0;32m    764\u001b[0m     past_key_values: Optional[torch\u001b[39m.\u001b[39mFloatTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    765\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m CausalLMOutputWithPast:\n\u001b[0;32m    767\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m past_key_values:\n\u001b[1;32m--> 768\u001b[0m         lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers(input_ids)\n\u001b[0;32m    769\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    770\u001b[0m         hidden_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m](input_ids)\n",
      "File \u001b[1;32mc:\\Users\\D4nk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\D4nk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\D4nk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\microsoft\\phi-1_5\\4a426d8015bef5a0cb3acff8d4474ee9ab4071d5\\modeling_mixformer_sequential.py:632\u001b[0m, in \u001b[0;36mParallelBlock.forward\u001b[1;34m(self, hidden_states, past_cache)\u001b[0m\n\u001b[0;32m    629\u001b[0m     attn_outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    631\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresid_dropout(attn_outputs)\n\u001b[1;32m--> 632\u001b[0m feed_forward_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresid_dropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states))\n\u001b[0;32m    634\u001b[0m hidden_states \u001b[39m=\u001b[39m attn_outputs \u001b[39m+\u001b[39m feed_forward_hidden_states \u001b[39m+\u001b[39m residual\n\u001b[0;32m    636\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\D4nk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\microsoft\\phi-1_5\\4a426d8015bef5a0cb3acff8d4474ee9ab4071d5\\modeling_mixformer_sequential.py:283\u001b[0m, in \u001b[0;36mMLP.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mFloatTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor:\n\u001b[0;32m    282\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(hidden_states)\n\u001b[1;32m--> 283\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(hidden_states)\n\u001b[0;32m    284\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(hidden_states)\n\u001b[0;32m    286\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\D4nk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/phi-1_5\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, trust_remote_code=True, torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "# tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./lotr/all.txt\",\n",
    "    block_size=1024,\n",
    ")\n",
    "print(dataset)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, trust_remote_code=True, torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "# model.save_pretrained(output_dir)\n",
    "\n",
    "# train_loader = DataLoader(encoded_dataset[\"train\"], batch_size=8, shuffle=True)\n",
    "# eval_loader = DataLoader(encoded_dataset[\"test\"], batch_size=8)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    # num_train_epochs=3,\n",
    "    # per_device_train_batch_size=8,\n",
    "    # per_device_eval_batch_size=8,\n",
    "    # warmup_steps=500,\n",
    "    # weight_decay=0.01,\n",
    "    # logging_dir=\"./logs\",\n",
    "    # logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    # eval_dataset=dataset[\"test\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf86b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_file_path = \"./lotr/the-fellowship-of-the-ring.txt\"\n",
    "# model_name = \"microsoft/phi-1_5\"\n",
    "# output_dir = \"./output\"\n",
    "# overwrite_output_dir = True\n",
    "# per_device_train_batch_size = 8\n",
    "# num_train_epochs = 50.0\n",
    "# save_steps = 50000\n",
    "\n",
    "# train(\n",
    "#     train_file_path=train_file_path,\n",
    "#     model_name=model_name,\n",
    "#     output_dir=output_dir,\n",
    "#     overwrite_output_dir=overwrite_output_dir,\n",
    "#     per_device_train_batch_size=per_device_train_batch_size,\n",
    "#     num_train_epochs=num_train_epochs,\n",
    "#     save_steps=save_steps,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef501948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_file_path = \"./lotr/the-fellowship-of-the-ring.txt\"\n",
    "# model_name = \"microsoft/phi-1_5\"\n",
    "# output_dir = \"./output\"\n",
    "# overwrite_output_dir = True\n",
    "# per_device_train_batch_size = 8\n",
    "# num_train_epochs = 50.0\n",
    "# save_steps = 50000\n",
    "\n",
    "# train(\n",
    "#     train_file_path=train_file_path,\n",
    "#     model_name=model_name,\n",
    "#     output_dir=output_dir,\n",
    "#     overwrite_output_dir=overwrite_output_dir,\n",
    "#     per_device_train_batch_size=per_device_train_batch_size,\n",
    "#     num_train_epochs=num_train_epochs,\n",
    "#     save_steps=save_steps,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9399ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\n",
    "#     \"text\",\n",
    "#     data_files={\n",
    "#         \"train\": [\n",
    "#             \"./lotr/the-fellowship-of-the-ring.txt\",\n",
    "#             \"./lotr/the-return-of-the-king.txt\",\n",
    "#             \"./lotr/the-two-towers.txt\",\n",
    "#         ]\n",
    "#     },\n",
    "# )\n",
    "\n",
    "\n",
    "# def load_dataset(file_path, tokenizer, block_size=128):\n",
    "#     dataset = TextDataset(\n",
    "#         tokenizer=tokenizer,\n",
    "#         file_path=file_path,\n",
    "#         block_size=block_size,\n",
    "#     )\n",
    "#     return dataset\n",
    "\n",
    "\n",
    "# print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "# tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "# small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "# small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c2db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer(\n",
    "#     \"write me a lord of the rings style short story\",\n",
    "#     return_tensors=\"pt\",\n",
    "#     return_attention_mask=False,\n",
    "# )\n",
    "# outputs = model.generate(**inputs, max_length=200)\n",
    "# text = tokenizer.batch_decode(outputs)[0]\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d03a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import (\n",
    "#     get_peft_config,\n",
    "#     get_peft_model,\n",
    "#     PromptTuningInit,\n",
    "#     PromptTuningConfig,\n",
    "#     TaskType,\n",
    "#     PeftType,\n",
    "#     PromptEncoderConfig,\n",
    "#     PrefixTuningConfig,\n",
    "#     LoraConfig,\n",
    "#     PeftConfig,\n",
    "# )\n",
    "\n",
    "# lora_peft_config = LoraConfig()\n",
    "\n",
    "# dynamic_padding = True\n",
    "\n",
    "\n",
    "# def tokenize_func(examples):\n",
    "#     return tokenizer(\n",
    "#         examples[\"input\"], truncation=True, max_length=1044\n",
    "#     )  # max_length=512,  padding=True\n",
    "\n",
    "\n",
    "# train_dataset_final = train_dataset.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc96dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "# model = prepare_model_for_int8_training(model)\n",
    "# model = get_peft_model(model, lora_peft_config)\n",
    "# training_args = model_training_args\n",
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     train_dataset=train_dataset_final,\n",
    "#     dataset_text_field=\"text\",\n",
    "#     max_seq_length=1044,\n",
    "#     tokenizer=tokenizer,\n",
    "#     args=model_training_args,\n",
    "#     packing=True,\n",
    "#     peft_config=lora_peft_config,\n",
    "# )\n",
    "# trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
